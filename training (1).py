# -*- coding: utf-8 -*-
"""training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DVD1ulyuAqI_InHSSg73ZQ3YK05TN-bS
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.

# %cd "/content/drive/MyDrive/PFA"

# !pip install tensorflow-io
# !pip install mtcnn
!pip install import-ipynb

import import_ipynb
import os
import cv2 as cv
import matplotlib.pyplot as plt
import numpy as np
import time
import tensorflow as tf
from keras.layers import Input
from tensorflow.keras.optimizers import Adam
from Discriminator import Discriminator , SpectralNormalization
from Generator import Generator
from DiscriminatorLosses import DiscrimnatorLosses
from GeneratorLosses import GeneratorLosses
from Data_preparation_loading import  Data_Preparation

def model():
    gen_input = Input(shape=(512, 512, 9))
    dis_input = Input(shape=(512, 512, 8))
    model_generator = Generator()
    model_disciminator = Discriminator()
    gen_model = model_generator.call(gen_input)
    dis_model = model_disciminator.call(dis_input)
    return gen_model, dis_model

def data_split(data):
    input_gen = data.total_input
    ground_truth = data.ground_truth
    batch_data = data.batch_data

    incomplete_image,sketch ,color ,mask,noise = batch_data[0],batch_data[1],batch_data[2],batch_data[3],\
                                                 batch_data[4]

    return input_gen,ground_truth,incomplete_image,sketch,color,mask,noise



def data_ditribution(input):
    data=tf.data.Dataset.from_tensor_slices((input)).batch(1,drop_remainder=True)
    return data

def apply_gradient(input_gen,mask,incomplete_image,ground_truth,sketch,color):
    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:

        output_gen = model_generator(input_gen)

        complete_image = incomplete_image + (mask * output_gen)

        batch_pos= tf.concat([ground_truth,sketch,color,mask],axis=-1)
        batch_neg = tf.concat([complete_image,sketch,color,mask], axis=-1)

        dis_real = model_discriminator(batch_pos)
        dis_fake = model_discriminator(batch_neg)

        gen_loss = GeneratorLosses().generator_loss(output_gen, ground_truth, complete_image, mask, dis_fake, dis_real)
        dis_loss = DiscrimnatorLosses().total_dis_loss(dis_real,dis_fake,model_discriminator,batch_pos,batch_neg,mask)

        generator_gradients = gen_tape.gradient(gen_loss,model_generator.trainable_variables)
        discriminator_gradients = disc_tape.gradient(dis_loss,model_discriminator.trainable_variables)

        generator_optimizer.apply_gradients(zip(generator_gradients,model_generator.trainable_variables))
        discriminator_optimizer.apply_gradients(zip(discriminator_gradients,model_discriminator.trainable_variables))


        return output_gen,gen_loss,dis_loss

def train_data(real_data, input_data, incomplete_data, mask_data, sketch_data, color_data):
    dis_losses = []
    gen_losses = []
    # print(real_data)
    for step, data in enumerate(zip(real_data, input_data, incomplete_data, mask_data, sketch_data, color_data)):
        output_gen, gen_loss, dis_loss = apply_gradient(data[1],data[3],data[2],data[0],data[4],data[5])


        gen_losses.append(gen_loss.numpy())
        dis_losses.append(dis_loss.numpy())

        # print(f"Training loss for step_num {step} ,gen_loss:{gen_loss:0.4f},dis_loss:{dis_loss:0.4f}")


    return gen_losses,dis_losses

# model_generator,model_discriminator = model()
generator_optimizer = Adam(1e-4, beta_1=0.1, beta_2=0.999)
discriminator_optimizer = Adam(1e-4, beta_1=0.1, beta_2=0.999)



model_generator = tf.keras.models.load_model('generator_model.h5', custom_objects={'SpectralNormalization': SpectralNormalization,'generator_loss' : GeneratorLosses().generator_loss})
model_discriminator = tf.keras.models.load_model('discrimintaor_model.h5', custom_objects={'SpectralNormalization': SpectralNormalization, 'total_dis_loss': DiscrimnatorLosses().total_dis_loss})

model_generator.compile(optimizer=generator_optimizer, loss=GeneratorLosses().generator_loss)
model_discriminator.compile(optimizer=discriminator_optimizer, loss=DiscrimnatorLosses().total_dis_loss)

def fit(epoch=1):
    # epochs_gen_losses, epochs_dis_losses = [], []
    # for epoch in range(epochs):
    batch_gen_losses, batch_dis_losses =[], []
    os.mkdir(f"model_weights/epoch_{epoch}")
    print(f"Start of epoch number : {epoch}")
    B_start_time = time.time()
    while  not data.last :
        start_time = time.time()
        input_gen,ground_truth,incomplete_image,sketch,color,mask,noise=data_split(data)
        input_data = data_ditribution(input_gen)
        incomplete_data = data_ditribution(incomplete_image)
        mask_data = data_ditribution(mask)
        real_data = data_ditribution(ground_truth)
        sketch_data = data_ditribution(sketch)
        color_data = data_ditribution(color)
        gen_losses,dis_losses = train_data(real_data, input_data, incomplete_data, mask_data, sketch_data, color_data)
        elapsed = time.time() - start_time
        minutes = int(elapsed // 60)
        seconds = int(elapsed % 60)
        batch_gen_losses.append(np.mean(gen_losses))
        batch_dis_losses.append(np.mean(dis_losses))
        print(f'Epoch {epoch} Batch {data.nb} : gen_loss: {np.mean(gen_losses):0.3f}  dis_loss: {np.mean(dis_losses):.3f} time:({minutes} min {seconds} sec)')
        model_generator.save_weights(f"model_weights/epoch_{epoch}/gen_model_epoch_{epoch}.h5")
        model_discriminator.save_weights(f"model_weights/epoch_{epoch}/dis_model_epoch_{epoch}.h5")
        model_generator.save('generator_model.h5')
        model_discriminator.save('discrimintaor_model.h5')
         # Save losses to file
        B_elapsed = time.time() - B_start_time
        B_minutes = int(B_elapsed // 60)
        B_seconds = int(B_elapsed % 60)
        with open(f"model_weights/epoch_{epoch}/losses.txt", "w") as f:
            f.write(f"Epoch {epoch}  : gen_loss: {np.mean(batch_gen_losses):0.3f}  dis_loss: {np.mean(batch_dis_losses):.3f} time:({B_minutes} min {B_seconds} sec)\n")
            f.write(f"Batch generator losses: {batch_gen_losses}\n")
            f.write(f"Batch discriminator losses: {batch_dis_losses}\n")
        print("models and losses saved")
        data.getNext()


    B_elapsed = time.time() - B_start_time
    B_minutes = int(B_elapsed // 60)
    B_seconds = int(B_elapsed % 60)
    # epochs_gen_losses.append(np.mean(batch_gen_losses))
    # epochs_dis_losses.append(np.mean(batch_dis_losses))
    print(f'Epoch {epoch}  : gen_loss: {np.mean(batch_gen_losses):0.3f}  dis_loss: {np.mean(batch_dis_losses):.3f} time:({B_minutes} min {B_seconds} sec)')
    with open(f"model_weights/epoch_{epoch}/losses.txt", "w") as f:
            f.write(f"Epoch {epoch}  : gen_loss: {np.mean(batch_gen_losses):0.3f}  dis_loss: {np.mean(batch_dis_losses):.3f} time:({B_minutes} min {B_seconds} sec)\n")
            f.write(f"Batch generator losses: {batch_gen_losses}\n")
            f.write(f"Batch discriminator losses: {batch_dis_losses}\n")

    return np.mean(batch_gen_losses) , np.mean(batch_gen_losses)
        # genereta_random_number =np.random.randint(0,4)
        # data_evaluation(genereta_random_number)

    # plot_history(epochs_gen_losses,epochs_dis_losses,epochs)

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/MyDrive/PFA"

# warnings
import warnings
warnings.filterwarnings('ignore')
# Epoch 2 avec nb = 8
path = "celebahq/CelebAMask-HQ/CelebA-HQ-img/"

data = Data_Preparation(path)

data.nb = 59
data.getNext()

num_epoch = 3
epochs_gen_losses , epochs_dis_losses = fit(num_epoch)