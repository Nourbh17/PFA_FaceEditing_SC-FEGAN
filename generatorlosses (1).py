# -*- coding: utf-8 -*-
"""GeneratorLosses.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pLxifJCViYOePd2_Po3xlBD610MH4wfc
"""

from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.models import Model
import tensorflow as tf

class GeneratorLosses():


    def __init__(self):
      self.pool_layers = ["block1_pool", "block2_pool", "block3_pool"]
      self.vgg = self.get_vgg16_layers(self.pool_layers)
    # gsn_loss function computes this generator loss. By negating the mean of the discriminator's output for the fake data,
    # the generator is encouraged to produce data that the discriminator will classify as real,
    # thereby minimizing the Wasserstein distance between the real and fake data distributions.
    def gsn_loss(self,dis_fake):
        gen_loss = - tf.reduce_mean(dis_fake)
        return gen_loss
    def per_pixel_loss(self,gen_output, ground_truth, mask, alpha):
        _, n_h, n_w, n_c = ground_truth.get_shape().as_list()

        term_1 = tf.reduce_sum(mask * (gen_output - ground_truth)) / (n_h * n_w * n_c)
        term_2 = tf.reduce_sum((1. - mask) * (gen_output - ground_truth)) / (n_h * n_w * n_c)

        loss = term_1 + alpha * term_2
        return loss

    def get_vgg16_layers(self,layers_names):
        vgg = VGG16(include_top=False, input_shape=(512, 512, 3), weights='imagenet')
        vgg.trainable = False
        outputs = [vgg.get_layer(name).output for name in layers_names]
        model = Model(inputs=vgg.input, outputs=outputs)
        return model

    def perceptual_loss(self,complete_image, ground_truth, gen_image):
        features_ground_truth = self.vgg(ground_truth)
        features_output_image = self.vgg(complete_image)
        features_gen_image = self.vgg(gen_image)


        assert len(features_ground_truth) == 3
        assert len(features_output_image) == 3
        assert len(features_gen_image) == 3

        term_1 = 0.
        term_2 = 0.
        for i in range(len(features_ground_truth)):
            _, n_h, n_w, n_c = features_ground_truth[i].get_shape().as_list()
            loss = tf.reduce_sum(features_output_image[i] - features_ground_truth[i]) / (n_h * n_w * n_c)
            term_1 += loss
            loss2 = tf.reduce_sum(features_gen_image[i] - features_ground_truth[i]) / (n_h * n_w * n_c)
            term_2 += loss2
        return term_1 + term_2

    def gram_matrix(self,tensor):
        """tensor shape is [batch,channels,width*height]"""
        """ output shape is [batch,channels ,channels] """
        assert len(tensor.get_shape().as_list()) == 3
        x = tf.transpose(tensor, (0, 2, 1))
        gram_matrix = tf.matmul(tensor, x)
        return gram_matrix

    def compute_style(self,gen_output_image, ground_truth_image):
      assert len(gen_output_image.shape) == 4
      assert len(ground_truth_image.shape) == 4

      batch_size, n_h, n_w, n_c = ground_truth_image.get_shape().as_list()
      # reshape tensor to [batch_size,width*height,channels]
      x = tf.reshape(gen_output_image, shape=[batch_size, n_h * n_w, n_c])
      x = tf.transpose(x, (0, 2, 1))
      # x = [batch_size,channels,width*height]

      # reshape tensor to [batch_size,width*height,channels]
      y = tf.reshape(ground_truth_image, shape=[batch_size, n_h * n_w, n_c])
      y = tf.transpose(y, (0, 2, 1))
      # y = [batch_size,channels,width*height]

      output_1 = self.gram_matrix(x)
      output_2 = self.gram_matrix(y)

      # output_style = tf.reduce_sum(output_1 - output_2) / (n_c * n_c)

      N_q = n_h * n_w

      output_style = tf.reduce_sum(tf.abs(output_1 - output_2)) / ( (n_c ** 2) * N_q )

      return output_style


    def style_loss(self,gen_output_image, ground_truth):
      features_ground_truth = self.vgg(ground_truth)
      features_output_image = self.vgg(gen_output_image)

      assert len(features_ground_truth) == 3
      assert len(features_output_image) == 3

      l_style_gen = 0.
      for i in range(len(features_ground_truth)):
        loss = self.compute_style(features_output_image[i], features_ground_truth[i])
        l_style_gen += loss
      return l_style_gen

    def total_variation_row_loss(self,complete_image, region):
      x_var = tf.reduce_sum(
          [tf.reduce_sum(complete_image[:, i + 1, j, :] - complete_image[:, i, j, :]) for i in region[1] for j in
          region[2]])
      x_var = x_var / (512 * 512 * 3)
      return x_var

    def total_variation_col_loss(self,complete_image, region):
      y_var = tf.reduce_sum(
        [tf.reduce_sum(complete_image[:, i, j + 1, :] - complete_image[:, i, j, :]) for i in region[1] for j in
         region[2]])
      y_var = y_var / (512 * 512 * 3)
      return y_var

    def total_variation_loss(self,complete_image, mask):
      completed = tf.multiply(complete_image, mask)
      zero = tf.constant(0, dtype=tf.float32)
      where = tf.not_equal(completed, zero)
      region = tf.where(where)

      x_var = self.total_variation_row_loss(complete_image, region)
      y_var = self.total_variation_col_loss(complete_image, region)

      loss = x_var + y_var
      return loss

    def gt_loss(self,dis_real):
     return tf.square(tf.reduce_mean(dis_real))

    def generator_loss(self,output_gen, ground_truth, complete_image, mask, dis_fake, dis_real):
      loss = self.per_pixel_loss(output_gen, ground_truth, mask, alpha=6.)
      percep_loss = self.perceptual_loss(complete_image, ground_truth,output_gen)
      style_1 = self.style_loss(output_gen, ground_truth)
      style_2 = self.style_loss(complete_image, ground_truth)
      total_variation = self.total_variation_loss(complete_image, mask)
      total_loss = loss + (0.05 * percep_loss) + (120 * (style_1 + style_2)) + (0.1 * total_variation)
      generator_loss = total_loss + (0.001 * self.gsn_loss(dis_fake)) + (0.001 * self.gt_loss(dis_real))
      return generator_loss

