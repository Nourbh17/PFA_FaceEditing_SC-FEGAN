# -*- coding: utf-8 -*-
"""Discriminator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_0sEQIhNlhUXxz3M5cUWP99jn1ZPTLer
"""

import tensorflow as tf
from keras.layers import Conv2D,LeakyReLU,Input
from keras.models import Model


# inputs = Input(shape=(512,512,3))

class Discriminator(Model):
    def __init__(self, ):
        super(Discriminator, self).__init__()
        self.x_1=SpectralNormalization(Conv2D(64*1, (3, 3),strides=1,padding="same", activation=LeakyReLU(alpha=0.1)))
        self.x_2=SpectralNormalization(Conv2D(64*2, (3, 3), strides=2,padding="same",activation=LeakyReLU(alpha=0.1)))
        self.x_3=SpectralNormalization(Conv2D(64*4, (3, 3), strides=2,padding="same",activation=LeakyReLU(alpha=0.1)))
        self.x_4=SpectralNormalization(Conv2D(64*4, (3, 3), strides=2,padding="same",activation=LeakyReLU(alpha=0.1)))
        self.x_5=SpectralNormalization(Conv2D(64*4, (3, 3), strides=2,padding="same",activation=LeakyReLU(alpha=0.1)))
        self.x_6=SpectralNormalization(Conv2D(64*4, (3, 3), strides=2,padding="same",activation=LeakyReLU(alpha=0.1)))

    def call(self, inputs, training=True, **kwargs):
        x=self.x_1(inputs)
        x=self.x_2(x)
        x=self.x_3(x)
        x=self.x_4(x)
        x=self.x_5(x)
        output=self.x_6(x)
        model=Model(inputs=inputs,outputs=output)
        return model

import tensorflow as tf  # TF 2.0


class SpectralNormalization(tf.keras.layers.Wrapper):
    def __init__(self, layer, iteration=1, eps=1e-12, training=True, **kwargs):
        self.iteration = iteration
        self.eps = eps
        self.do_power_iteration = training
        if not isinstance(layer, tf.keras.layers.Layer):
            raise ValueError(
                'Please initialize `TimeDistributed` layer with a '
                '`Layer` instance. You passed: {input}'.format(input=layer))
        super(SpectralNormalization, self).__init__(layer, **kwargs)

    def build(self, input_shape):
        self.layer.build(input_shape)

        self.w = self.layer.kernel
        self.w_shape = self.w.shape.as_list()

        self.v = self.add_weight(shape=(1, self.w_shape[0] * self.w_shape[1] * self.w_shape[2]),
                                 initializer=tf.initializers.TruncatedNormal(stddev=0.02),
                                 trainable=False,
                                 name='sn_v',
                                 dtype=tf.float32)

        self.u = self.add_weight(shape=(1, self.w_shape[-1]),
                                 initializer=tf.initializers.TruncatedNormal(stddev=0.02),
                                 trainable=False,
                                 name='sn_u',
                                 dtype=tf.float32)

        super(SpectralNormalization, self).build()

    def call(self, inputs):
        self.update_weights()
        output = self.layer(inputs)
        self.restore_weights()  # Restore weights because of this formula "W = W - alpha * W_SN`"
        return output

    def update_weights(self):
        w_reshaped = tf.reshape(self.w, [-1, self.w_shape[-1]])

        u_hat = self.u
        v_hat = self.v  # init v vector

        if self.do_power_iteration:
            for _ in range(self.iteration):
                v_ = tf.matmul(u_hat, tf.transpose(w_reshaped))
                v_hat = v_ / (tf.reduce_sum(v_**2)**0.5 + self.eps)

                u_ = tf.matmul(v_hat, w_reshaped)
                u_hat = u_ / (tf.reduce_sum(u_**2)**0.5 + self.eps)

        sigma = tf.matmul(tf.matmul(v_hat, w_reshaped), tf.transpose(u_hat))
        self.u.assign(u_hat)
        self.v.assign(v_hat)

        self.layer.kernel.assign(self.w / sigma)

    def restore_weights(self):
        self.layer.kernel.assign(self.w)

# class conv2d_layer(tf.keras.layers.Layer):
#     def __init__(self, out_channels, kernel_size, stride=1, padding='valid', dilation=1, pad_type='zero', activation='lrelu', norm='none', sn=False):
#         super(conv2d_layer, self).__init__()


#         # Initialize the padding scheme
#         if pad_type == 'reflect':
#             self.pad = tf.keras.layers.Lambda(lambda x: tf.pad(x, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='REFLECT'))
#         elif pad_type == 'replicate':
#             self.pad = tf.keras.layers.Lambda(lambda x: tf.pad(x, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='SYMMETRIC'))
#         elif pad_type == 'zero':
#             self.pad = tf.keras.layers.ZeroPadding2D(padding)
#         else:
#             assert 0, "Unsupported padding type: {}".format(pad_type)

#         # Initialize the normalization type
#         if norm == 'bn':
#             self.norm = tf.keras.layers.BatchNormalization()
#         elif norm == 'in':
#             self.norm = tf.keras.layers.LayerNormalization()
#         elif norm == 'ln':
#             self.norm = tf.keras.layers.LayerNormalization(axis=[1, 2])
#         elif norm == 'none':
#             self.norm = None
#         else:
#             assert 0, "Unsupported normalization: {}".format(norm)

#         # Initialize the activation function
#         if activation == 'relu':
#             self.activation = tf.keras.layers.ReLU()
#         elif activation == 'lrelu':
#             self.activation = tf.keras.layers.LeakyReLU(0.2)
#         elif activation == 'prelu':
#             self.activation = tf.keras.layers.PReLU()
#         elif activation == 'selu':
#             self.activation = tf.keras.layers.SELU()
#         elif activation == 'tanh':
#             self.activation = tf.keras.activations.tanh
#         elif activation == 'sigmoid':
#             self.activation = tf.keras.activations.sigmoid
#         elif activation == 'none':
#             self.activation = None
#         else:
#             assert 0, "Unsupported activation: {}".format(activation)

#         # Initialize the convolution layers
#         if sn:
#             self.conv2d = SpectralNorm(tf.keras.layers.Conv2D(out_channels, kernel_size, strides=stride, padding='valid', dilation_rate=dilation, use_bias=False))
#         else:
#             self.conv2d = tf.keras.layers.Conv2D(out_channels, kernel_size, strides=stride, padding='valid', dilation_rate=dilation, use_bias=False)

#     def call(self, x):
#         x = self.pad(x)
#         x = self.conv2d(x)
#         if self.norm:
#             x = self.norm(x)
#         if self.activation:
#             x = self.activation(x)
#         return x

# class SpectralNorma(tf.keras.layers.Wrapper):
#   def __init__(self, layer, iteration=1, name="weight", **kwargs):
#     super(SpectralNorm, self).__init__(layer, **kwargs)
#     self.iteration = iteration
#     self.name = name

#   def build(self, input_shape):
#     if not hasattr(self.layer, self.name):
#       raise ValueError(f"Layer {self.layer.name} does not have attribute {self.name}")
#     w = getattr(self.layer, self.name)
#     self.w_bar = self.add_weight(
#         shape=w.shape,
#         name=f"{self.name}_bar",
#         initializer=tf.keras.initializers.TruncatedNormal(stddev=1.0),
#         trainable=True
#     )
#     self.u = self.add_weight(
#         shape=(w.shape[0], 1),
#         name=f"{self.name}_u",
#         initializer=tf.random.normal_initializer(mean=0.0, stddev=1.0),
#         trainable=False
#     )
#     self.v = self.add_weight(
#         shape=(1, w.shape[1]),
#         name=f"{self.name}_v",
#         initializer=tf.random.normal_initializer(mean=0.0, stddev=1.0),
#         trainable=False
#     )
#     super(SpectralNorm, self).build(input_shape)

#   def call(self, inputs, training=None):
#     if training:
#       self._update_u_v(inputs)
#     sigma = tf.tensordot(self.u, tf.matmul(self.w_bar, self.v), axes=([0], [1]))
#     self.layer.setattr(self.name, self.w_bar / sigma)
#     return self.layer(inputs)

#   def _update_u_v(self, inputs):
#     w_reshaped = tf.reshape(self.w_bar, (-1, self.w_bar.shape[1]))
#     for _ in range(self.iteration):
#       v = tf.nn.l2_normalize(tf.matmul(tf.transpose(w_reshaped), self.u), axis=0)
#       self.u.assign(tf.nn.l2_normalize(tf.matmul(w_reshaped, v), axis=1))